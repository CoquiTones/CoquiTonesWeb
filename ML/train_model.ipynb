{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# don\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import zipfile\n",
    "import csv\n",
    "import shutil\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,        \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script for extracting data from https://figshare.com/articles/dataset/Sounds_of_the_Eleutherodactylus_frog_community_from_Puerto_Rico/806302?file=3104183\n",
    "Unzips all the zips from root\n",
    "Simply Unzip downloaded file and provide path root of folder\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def extract_zip_file(file_path, extract_to):\n",
    "    \"\"\"\n",
    "    Extracts a single zip file to a specified directory.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the zip file.\n",
    "        extract_to (str): Path to the directory where the zip file will be extracted.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "        # print(f\"{os.path.basename(file_path)} extracted to {os.path.abspath(extract_to)}\")\n",
    "\n",
    "\n",
    "def extract_zip_files(zip_folder, extract_to):\n",
    "    \"\"\"\n",
    "    Extracts all zip files from a folder to a specified directory using threading.\n",
    "\n",
    "    Args:\n",
    "        zip_folder (str): Path to the folder containing zip files.\n",
    "        extract_to (str): Path to the directory where zip files will be extracted.\n",
    "    \"\"\"\n",
    "    # Make sure the extraction directory exists\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "    data_file = \"FrequencyRange_by_species_and_site_Averages.csv\"\n",
    "    shutil.copyfile(os.path.join(zip_folder, data_file) , os.path.join(extract_to, data_file))\n",
    "\n",
    "    # List all zip files in the folder\n",
    "    zip_files = [\n",
    "        os.path.join(zip_folder, item)\n",
    "        for item in os.listdir(zip_folder)\n",
    "        if item.endswith(\".zip\")\n",
    "    ]\n",
    "\n",
    "    # Use ThreadPoolExecutor to extract zip files concurrently\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [\n",
    "            executor.submit(extract_zip_file, zip_file, extract_to)\n",
    "            for zip_file in zip_files\n",
    "        ]\n",
    "\n",
    "        # Wait for all futures to complete\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "    \n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ccea4413e74aba8685cf562ec2a54f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Filepath:', placeholder='Path to Extracted Dataset')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e24d43b1aa4ade87275845113b8ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Ouput FP:', placeholder='Output path of Extracted Sets')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input stored: \"A:\\Documents\\Capstone\\Dataset\\806302\"\n",
      "Output stored: \"A:\\Documents\\Capstone\\Dataset\\806302\"\n",
      "Output stored: \"A:\\Documents\\Capstone\\Dataset\\E\"\n",
      "Output stored: \"A:\\Documents\\Capstone\\Dataset\\Ex\"\n",
      "Output stored: \"A:\\Documents\\Capstone\\Dataset\\Ext\"\n",
      "Output stored: \"A:\\Documents\\Capstone\\Dataset\\Extr\"\n",
      "Output stored: \"A:\\Documents\\Capstone\\Dataset\\Extra\"\n",
      "Output stored: \"A:\\Documents\\Capstone\\Dataset\\Extrac\"\n",
      "Output stored: \"A:\\Documents\\Capstone\\Dataset\\Extract\"\n",
      "Output stored: \"A:\\Documents\\Capstone\\Dataset\\Extracte\"\n",
      "Output stored: \"A:\\Documents\\Capstone\\Dataset\\Extracted\"\n",
      "Input stored: \"A:\\\\Documents\\Capstone\\Dataset\\806302\"\n",
      "Input stored: \"A:\\\\Documents\\\\Capstone\\Dataset\\806302\"\n",
      "Input stored: \"A:\\\\Documents\\\\Capstone\\\\Dataset\\806302\"\n",
      "Input stored: \"A:\\\\Documents\\\\Capstone\\\\Dataset\\\\806302\"\n",
      "Output stored: \"A:\\\\Documents\\Capstone\\Dataset\\Extracted\"\n",
      "Output stored: \"A:\\\\Documents\\\\Capstone\\Dataset\\Extracted\"\n",
      "Output stored: \"A:\\\\Documents\\\\Capstone\\\\Dataset\\Extracted\"\n",
      "Output stored: \"A:\\\\Documents\\\\Capstone\\\\Dataset\\\\Extracted\"\n",
      "Input stored: \"A:\\\\Documents\\\\Capstone\\\\Dataset\\\\806302\\\"\n",
      "Input stored: \"A:\\\\Documents\\\\Capstone\\\\Dataset\\\\806302\\\\\"\n",
      "Output stored: \"A:\\\\Documents\\\\Capstone\\\\Dataset\\\\Extracted\\\"\n",
      "Output stored: \"A:\\\\Documents\\\\Capstone\\\\Dataset\\\\Extracted\\\\\"\n",
      "Input stored: \"A:\\Documents\\Capstone\\Dataset\\806302\"\n",
      "Input stored: A:\\Documents\\Capstone\\Dataset\\806302\"\n",
      "Input stored: A:\\Documents\\Capstone\\Dataset\\806302\n",
      "Output stored: A:\\\\Documents\\\\Capstone\\\\Dataset\\\\Extracted\\\\\"\n",
      "Output stored: A:\\\\Documents\\\\Capstone\\\\Dataset\\\\Extracted\\\\\n",
      "Input stored: A:\\\\Documents\\Capstone\\Dataset\\806302\n",
      "Input stored: A:\\\\Documents\\\\Capstone\\Dataset\\806302\n",
      "Input stored: A:\\\\Documents\\\\Capstone\\Dataset\\\\806302\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create a Text widget\n",
    "text_widget_filepath = widgets.Text(\n",
    "    value=\"\", placeholder=\"Path to Extracted Dataset\", description=\"Filepath:\", disabled=False\n",
    ")\n",
    "text_widget_extractTo = widgets.Text(\n",
    "    value=\"\",\n",
    "    placeholder=\"Output path of Extracted Sets\",\n",
    "    description=\"Ouput FP:\",\n",
    "    disabled=False,\n",
    ")\n",
    "# Variable to store the text\n",
    "filepath = \"\"\n",
    "extractTo = \"\"\n",
    "\n",
    "# Function to handle text changes\n",
    "def handle_text_change_filepath(change):\n",
    "    global filepath\n",
    "    filepath = change[\"new\"]\n",
    "    print(f\"Input stored: {filepath}\")\n",
    "\n",
    "\n",
    "# Function to handle text changes\n",
    "def handle_text_change_extractTo(change):\n",
    "    global extractTo\n",
    "    extractTo = change[\"new\"]\n",
    "    print(f\"Output stored: {extractTo}\")\n",
    "\n",
    "\n",
    "# Set up the event handler\n",
    "text_widget_filepath.observe(handle_text_change_filepath, names=\"value\")\n",
    "text_widget_extractTo.observe(handle_text_change_extractTo, names=\"value\")\n",
    "# Display the widget\n",
    "display(text_widget_filepath)\n",
    "display(text_widget_extractTo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Filepath:  A:\\\\Documents\\\\Capstone\\Dataset\\\\806302\n",
      "Extracting to :  A:\\\\Documents\\\\Capstone\\\\Dataset\\\\Extracted\\\\\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"Using Filepath: \", filepath)\n",
    "print(\"Extracting to : \", extractTo)\n",
    "extract_zip_files(filepath, extractTo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readAveragesData(path: str):\n",
    "\n",
    "    data = []\n",
    "    # Read the CSV file and store the data in a list of dictionaries\n",
    "    with open(path, \"r\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_csv(data_dir, output) -> None:\n",
    "\n",
    "    \n",
    "    averagesData = readAveragesData(\n",
    "        os.path.join(data_dir, \"FrequencyRange_by_species_and_site_Averages.csv\")\n",
    "    )\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # Iterate through each subfolder\n",
    "    for siteDataSet in os.listdir(data_dir):\n",
    "        site_folder = os.path.join(data_dir, siteDataSet)\n",
    "        if os.path.isdir(site_folder):\n",
    "            # example siteId  \"Site01-1\" such that the 4-6 index represents the site id; in this case 01\n",
    "            siteId = int(siteDataSet[4:6])\n",
    "            SiteData = [\n",
    "                averageClassification\n",
    "                for averageClassification in averagesData\n",
    "                if int(averageClassification[\"SiteID\"]) == siteId\n",
    "            ]\n",
    "\n",
    "            classifications = \", \".join(\n",
    "                [classification[\"Species\"] for classification in SiteData]\n",
    "            )\n",
    "            for audio_recording in os.listdir(site_folder):\n",
    "                if audio_recording.endswith(\".wav\"):\n",
    "                    audio_recording_abs_path = os.path.abspath(\n",
    "                        os.path.join(site_folder, audio_recording)\n",
    "                    )\n",
    "\n",
    "                    data.append([siteId, audio_recording_abs_path, classifications])\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        data,\n",
    "        columns=[\n",
    "            \"siteId\",\n",
    "            \"filename\",\n",
    "            \"species\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    df.to_csv(output, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = extractTo\n",
    "output = \"processed/processed.csv\"\n",
    "prepare_csv(data_dir, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_path):\n",
    "    \"\"\"\n",
    "    Extract features from audio file using librosa.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the audio file.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Extracted features.\n",
    "    \"\"\"\n",
    "    audio, sr = librosa.load(file_path)\n",
    "    result = np.array([])\n",
    "\n",
    "    # MFCC\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=audio, sr=sr).T, axis=0)\n",
    "    result = np.hstack((result, mfccs))\n",
    "\n",
    "    # Chroma\n",
    "    stft = np.abs(librosa.stft(audio))\n",
    "    chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sr).T, axis=0)\n",
    "    result = np.hstack((result, chroma))\n",
    "\n",
    "    # Mel-scaled spectrogram\n",
    "    mel = np.mean(librosa.feature.melspectrogram(y=audio, sr=sr).T, axis=0)\n",
    "    result = np.hstack((result, mel))\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def process_data(data_csv_path):\n",
    "    \"\"\"Read Csv with fileanme and generate spectrogram for each sample\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: dataframe with all data\n",
    "    \"\"\"\n",
    "    # data_csv_path = sys.argv[1]\n",
    "\n",
    "    df = pd.read_csv(data_csv_path)\n",
    "\n",
    "    # Initialize a list to store the results\n",
    "    spectrograms = []\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [\n",
    "            executor.submit(extract_features, row[\"filename\"])\n",
    "            for _, row in df.iterrows()\n",
    "        ]\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                spectrogram = future.result()\n",
    "                spectrograms.append(spectrogram)\n",
    "            except Exception as exc:\n",
    "                print(f\"Generated an exception: {exc}\")\n",
    "\n",
    "    # Convert the list of spectrograms into a DataFrame\n",
    "    spectrogram_df = pd.DataFrame(spectrograms)\n",
    "\n",
    "    # Concatenate the original DataFrame with the new DataFrame containing spectrograms\n",
    "    df = pd.concat([df, spectrogram_df], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>siteId</th>\n",
       "      <th>filename</th>\n",
       "      <th>species</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>...</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A:\\Documents\\Capstone\\Dataset\\Extracted\\Site01...</td>\n",
       "      <td>E. coqui - co, E. coqui - qui, E. wightmanae</td>\n",
       "      <td>-236.219101</td>\n",
       "      <td>63.262177</td>\n",
       "      <td>-57.943291</td>\n",
       "      <td>49.081966</td>\n",
       "      <td>10.479619</td>\n",
       "      <td>48.403107</td>\n",
       "      <td>-37.035892</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.106068e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A:\\Documents\\Capstone\\Dataset\\Extracted\\Site01...</td>\n",
       "      <td>E. coqui - co, E. coqui - qui, E. wightmanae</td>\n",
       "      <td>-245.313828</td>\n",
       "      <td>59.316032</td>\n",
       "      <td>-79.574265</td>\n",
       "      <td>33.324780</td>\n",
       "      <td>11.406502</td>\n",
       "      <td>31.833162</td>\n",
       "      <td>-28.828608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.021079e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>A:\\Documents\\Capstone\\Dataset\\Extracted\\Site01...</td>\n",
       "      <td>E. coqui - co, E. coqui - qui, E. wightmanae</td>\n",
       "      <td>-209.789093</td>\n",
       "      <td>46.209599</td>\n",
       "      <td>-96.112762</td>\n",
       "      <td>31.747490</td>\n",
       "      <td>32.042698</td>\n",
       "      <td>39.617424</td>\n",
       "      <td>-38.199471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>1.468620e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>A:\\Documents\\Capstone\\Dataset\\Extracted\\Site01...</td>\n",
       "      <td>E. coqui - co, E. coqui - qui, E. wightmanae</td>\n",
       "      <td>-219.443344</td>\n",
       "      <td>46.161240</td>\n",
       "      <td>-95.875694</td>\n",
       "      <td>47.622898</td>\n",
       "      <td>17.900232</td>\n",
       "      <td>44.626976</td>\n",
       "      <td>-33.330956</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>4.669851e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>A:\\Documents\\Capstone\\Dataset\\Extracted\\Site01...</td>\n",
       "      <td>E. coqui - co, E. coqui - qui, E. wightmanae</td>\n",
       "      <td>-215.081345</td>\n",
       "      <td>45.414715</td>\n",
       "      <td>-97.795143</td>\n",
       "      <td>36.125481</td>\n",
       "      <td>27.784109</td>\n",
       "      <td>37.399685</td>\n",
       "      <td>-34.257126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>9.516400e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 163 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   siteId                                           filename  \\\n",
       "0       1  A:\\Documents\\Capstone\\Dataset\\Extracted\\Site01...   \n",
       "1       1  A:\\Documents\\Capstone\\Dataset\\Extracted\\Site01...   \n",
       "2       1  A:\\Documents\\Capstone\\Dataset\\Extracted\\Site01...   \n",
       "3       1  A:\\Documents\\Capstone\\Dataset\\Extracted\\Site01...   \n",
       "4       1  A:\\Documents\\Capstone\\Dataset\\Extracted\\Site01...   \n",
       "\n",
       "                                        species           0          1  \\\n",
       "0  E. coqui - co, E. coqui - qui, E. wightmanae -236.219101  63.262177   \n",
       "1  E. coqui - co, E. coqui - qui, E. wightmanae -245.313828  59.316032   \n",
       "2  E. coqui - co, E. coqui - qui, E. wightmanae -209.789093  46.209599   \n",
       "3  E. coqui - co, E. coqui - qui, E. wightmanae -219.443344  46.161240   \n",
       "4  E. coqui - co, E. coqui - qui, E. wightmanae -215.081345  45.414715   \n",
       "\n",
       "           2          3          4          5          6  ...       150  \\\n",
       "0 -57.943291  49.081966  10.479619  48.403107 -37.035892  ...  0.000009   \n",
       "1 -79.574265  33.324780  11.406502  31.833162 -28.828608  ...  0.000018   \n",
       "2 -96.112762  31.747490  32.042698  39.617424 -38.199471  ...  0.000082   \n",
       "3 -95.875694  47.622898  17.900232  44.626976 -33.330956  ...  0.000033   \n",
       "4 -97.795143  36.125481  27.784109  37.399685 -34.257126  ...  0.000053   \n",
       "\n",
       "        151       152       153       154       155       156       157  \\\n",
       "0  0.000003  0.000003  0.000003  0.000003  0.000003  0.000003  0.000003   \n",
       "1  0.000008  0.000007  0.000007  0.000006  0.000006  0.000007  0.000008   \n",
       "2  0.000054  0.000046  0.000040  0.000034  0.000031  0.000039  0.000040   \n",
       "3  0.000031  0.000027  0.000014  0.000013  0.000016  0.000019  0.000020   \n",
       "4  0.000042  0.000035  0.000026  0.000022  0.000019  0.000025  0.000025   \n",
       "\n",
       "        158           159  \n",
       "0  0.000001  1.106068e-07  \n",
       "1  0.000003  2.021079e-07  \n",
       "2  0.000021  1.468620e-06  \n",
       "3  0.000009  4.669851e-07  \n",
       "4  0.000012  9.516400e-07  \n",
       "\n",
       "[5 rows x 163 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_dir = output\n",
    "df = process_data(csv_dir)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[0;32m      2\u001b[0m     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspecies\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m )  \u001b[38;5;66;03m# Adjust this to include only feature columns\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Convert all column names to strings\u001b[39;00m\n\u001b[0;32m      5\u001b[0m x\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "x = df.drop(\n",
    "    columns=[\"filename\", \"species\"]\n",
    ")  # Adjust this to include only feature columns\n",
    "# Convert all column names to strings\n",
    "x.columns = x.columns.astype(str)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"species\"]\n",
    "\n",
    "# Encode the target labels as integers\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=600, max_depth=18, min_samples_leaf=3)\n",
    "\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict_proba(\n",
    "    x_test,\n",
    ")\n",
    "\n",
    "accuracy = roc_auc_score(y_test, y_pred, multi_class=\"ovr\")\n",
    "print(\"Accuracy :\", accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../Backend/trainedRF.pkl\", \"wb\") as f:\n",
    "    pickle.dump(classifier, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
