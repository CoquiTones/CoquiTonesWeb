{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Model Training Notebook\n",
            "\n",
            "This notebook can be used for downloading the [coqui acoustic dataset](https://figshare.com/articles/dataset/Sounds_of_the_Eleutherodactylus_frog_community_from_Puerto_Rico/806302) provided by researcher Luis Villanueva-Rivera and creating a machine learning model using Random Forest Classification algorithm. \n",
            "\n",
            "# Requirements\n",
            "\n",
            "Created python virtual environment with `backend\\requirements.txt` dependencies\n",
            "\n",
            "# Usage\n",
            "\n",
            "Play the cells one by one according to your needs. You may choose to skip certain steps if already done previously."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# -*- coding: utf-8 -*-\n",
            "\n",
            "# don\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "import librosa\n",
            "import os\n",
            "import zipfile\n",
            "import csv\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.preprocessing import LabelEncoder\n",
            "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
            "from sklearn.metrics import (\n",
            "    roc_auc_score,        \n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Download The Dataset\n",
            "\n",
            "If you have not already downloaded the dataset uncomment the `urlretrieve` line from the cell below\n",
            "\n",
            "Otherwise if you already have the zip folder, place it `machine_learning\\dataset\\coqui_dataset_Luis_Villanueva-Rivera.zip` and make sure to rename the zip folder accordingly. \n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from urllib.request import urlretrieve\n",
            "\n",
            "\n",
            "def extract_zip_file(file_path, extract_to):\n",
            "    \"\"\"\n",
            "    Extracts a single zip file to a specified directory.\n",
            "\n",
            "    Args:\n",
            "        file_path (str): Path to the zip file.\n",
            "        extract_to (str): Path to the directory where the zip file will be extracted.\n",
            "    \"\"\"\n",
            "    with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
            "        zip_ref.extractall(extract_to)\n",
            "        # print(f\"{os.path.basename(file_path)} extracted to {os.path.abspath(extract_to)}\")\n",
            "\n",
            "\n",
            "def extract_zip_files(source_data_zip, destination_folder):\n",
            "    \"\"\"\n",
            "    Extracts all zip files from a folder to a specified directory using threading.\n",
            "\n",
            "    Args:\n",
            "        zip_folder (str): Path to the folder containing zip files.\n",
            "        extract_to (str): Path to the directory where zip files will be extracted.\n",
            "    \"\"\"\n",
            "    # Make sure the extraction directory exists\n",
            "    os.makedirs(destination_folder, exist_ok=True)\n",
            "\n",
            "    extract_zip_file(source_data_zip, destination_folder)\n",
            "    # List all zip files in the folder\n",
            "\n",
            "    zip_files = [\n",
            "        os.path.join(destination_folder, item)\n",
            "        for item in os.listdir(destination_folder)\n",
            "        if item.endswith(\".zip\")\n",
            "    ]\n",
            "\n",
            "    # Use ThreadPoolExecutor to extract zip files concurrently\n",
            "    with ThreadPoolExecutor() as executor:\n",
            "        futures = [\n",
            "            executor.submit(extract_zip_file, zip_file, destination_folder)\n",
            "            for zip_file in zip_files\n",
            "        ]\n",
            "\n",
            "        # Wait for all futures to complete\n",
            "        for future in futures:\n",
            "            future.result()\n",
            "\n",
            "    print(\"Done.\")\n",
            "\n",
            "\n",
            "def remove_zipped_files(extraction_destination_filepath):\n",
            "\n",
            "    zip_files = [\n",
            "        os.path.join(extraction_destination_filepath, item)\n",
            "        for item in os.listdir(extraction_destination_filepath)\n",
            "        if item.endswith(\".zip\")\n",
            "    ]\n",
            "\n",
            "    for file in zip_files:\n",
            "        os.remove(file)\n",
            "\n",
            "\n",
            "\n",
            "source_data_filepath = \"dataset\" + os.sep + \"coqui_dataset_Luis_Villanueva-Rivera.zip\"\n",
            "dataset_source = \"https://figshare.com/ndownloader/articles/806302/versions/18\"\n",
            "extraction_destination_filepath = \"dataset\" + os.sep + \"extracted\"\n",
            "\n",
            "\n",
            "# urlretrieve(dataset_source, source_data_filepath)\n",
            "\n",
            "# extract_zip_files(source_data_filepath, extraction_destination_filepath)\n",
            "# remove_zipped_files(extraction_destination_filepath)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Data Preparation \n",
            "\n",
            "Generate a csv associating each audio sample to the corresponding coqui identified in it. A single file can have multiple coqui's identified. We use the `FrequencyRange_by_species_and_site_Averages.csv` to create this report"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def readClassifiedReportData(path: str):\n",
            "\n",
            "    data = []\n",
            "    # Read the CSV file and store the data in a list of dictionaries\n",
            "    with open(path, \"r\") as file:\n",
            "        reader = csv.DictReader(file)\n",
            "        for row in reader:\n",
            "            data.append(row)\n",
            "\n",
            "    return data\n",
            "\n",
            "averagesData = readClassifiedReportData(\n",
            "    os.path.join(\n",
            "        extraction_destination_filepath,\n",
            "        \"FrequencyRange_by_species_and_site_Averages.csv\",\n",
            "    )\n",
            ")\n",
            "pd.options.display.max_columns = 250 #Changes the number of columns diplayed (default is 20)\n",
            "pd.options.display.max_rows = 250 #Changes the number of rows diplayed (default is 60)\n",
            "df = pd.DataFrame(averagesData)\n",
            "df = df.sort_values(by=[\"MinFreq (Hz)\"])\n",
            "\n",
            "df"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def prepare_csv(extraction_destination_filepath, output) -> None:\n",
            "\n",
            "    averagesData = readClassifiedReportData(\n",
            "        os.path.join(\n",
            "            extraction_destination_filepath,\n",
            "            \"FrequencyRange_by_species_and_site_Averages.csv\",\n",
            "        )\n",
            "    )\n",
            "\n",
            "    data = []\n",
            "\n",
            "    # Iterate through each subfolder\n",
            "    for siteDataSet in os.listdir(extraction_destination_filepath):\n",
            "        site_folder = os.path.join(extraction_destination_filepath, siteDataSet)\n",
            "        if os.path.isdir(site_folder):\n",
            "            # example siteId  \"Site01-1\" such that the 4-6 index represents the site id; in this case 01\n",
            "            siteId = int(siteDataSet[4:6])\n",
            "            SiteData = [\n",
            "                averageClassification\n",
            "                for averageClassification in averagesData\n",
            "                if int(averageClassification[\"SiteID\"]) == siteId\n",
            "            ]\n",
            "\n",
            "            classifications = \", \".join(\n",
            "                [classification[\"Species\"] for classification in SiteData]\n",
            "            )\n",
            "            for audio_recording in os.listdir(site_folder):\n",
            "                if audio_recording.endswith(\".wav\"):\n",
            "                    audio_recording_abs_path = os.path.abspath(\n",
            "                        os.path.join(site_folder, audio_recording)\n",
            "                    )\n",
            "\n",
            "                    data.append([siteId, audio_recording_abs_path, classifications])\n",
            "\n",
            "    # Create DataFrame\n",
            "    df = pd.DataFrame(\n",
            "        data,\n",
            "        columns=[\n",
            "            \"siteId\",\n",
            "            \"filename\",\n",
            "            \"species\",\n",
            "        ],\n",
            "    )\n",
            "\n",
            "    df.to_csv(output, index=False)\n",
            "\n",
            "\n",
            "\n",
            "output = \"processed/processed.csv\"\n",
            "\n",
            "prepare_csv(extraction_destination_filepath, output)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Data Processing\n",
            "\n",
            "Generate a Spectrogram for every audio sample. "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def extract_features(file_path):\n",
            "    \"\"\"\n",
            "    Extract features from audio file using librosa.\n",
            "\n",
            "    Args:\n",
            "        file_path (str): Path to the audio file.\n",
            "\n",
            "    Returns:\n",
            "        np.array: Extracted features.\n",
            "    \"\"\"\n",
            "    audio, sr = librosa.load(file_path)\n",
            "    result = np.array([])\n",
            "\n",
            "    # MFCC\n",
            "    mfccs = np.mean(librosa.feature.mfcc(y=audio, sr=sr).T, axis=0)\n",
            "    result = np.hstack((result, mfccs))\n",
            "\n",
            "    # Chroma\n",
            "    stft = np.abs(librosa.stft(audio))\n",
            "    chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sr).T, axis=0)\n",
            "    result = np.hstack((result, chroma))\n",
            "\n",
            "    # Mel-scaled spectrogram\n",
            "    mel = np.mean(librosa.feature.melspectrogram(y=audio, sr=sr).T, axis=0)\n",
            "    result = np.hstack((result, mel))\n",
            "\n",
            "    return result\n",
            "\n",
            "\n",
            "def process_data(data_csv_path):\n",
            "    \"\"\"Read Csv with fileanme and generate spectrogram for each sample\n",
            "\n",
            "    Returns:\n",
            "        DataFrame: dataframe with all data\n",
            "    \"\"\"\n",
            "    # data_csv_path = sys.argv[1]\n",
            "\n",
            "    df = pd.read_csv(data_csv_path)\n",
            "\n",
            "    # Initialize a list to store the results\n",
            "    spectrograms = []\n",
            "\n",
            "    with ThreadPoolExecutor() as executor:\n",
            "        futures = [\n",
            "            executor.submit(extract_features, row[\"filename\"])\n",
            "            for _, row in df.iterrows()\n",
            "        ]\n",
            "\n",
            "        for future in as_completed(futures):\n",
            "            try:\n",
            "                spectrogram = future.result()\n",
            "                spectrograms.append(spectrogram)\n",
            "            except Exception as exc:\n",
            "                print(f\"Generated an exception: {exc}\")\n",
            "\n",
            "    # Convert the list of spectrograms into a DataFrame\n",
            "    spectrogram_df = pd.DataFrame(spectrograms)\n",
            "\n",
            "    # Concatenate the original DataFrame with the new DataFrame containing spectrograms\n",
            "    df = pd.concat([df, spectrogram_df], axis=1)\n",
            "\n",
            "    return df\n",
            "\n",
            "\n",
            "csv_dir = output\n",
            "\n",
            "df = process_data(csv_dir)\n",
            "df.head()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "x = df.drop(\n",
            "    columns=[\"filename\", \"species\"]\n",
            ")  # Adjust this to include only feature columns\n",
            "# Convert all column names to strings\n",
            "x.columns = x.columns.astype(str)\n",
            "\n",
            "x"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "y = df[\"species\"]\n",
            "\n",
            "# Encode the target labels as integers\n",
            "label_encoder = LabelEncoder()\n",
            "y = label_encoder.fit_transform(y)\n",
            "\n",
            "le_bron = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
            "le_bron\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
            "\n",
            "classifier = RandomForestClassifier(n_estimators=600, max_depth=18, min_samples_leaf=3)\n",
            "\n",
            "classifier.fit(x_train, y_train)\n",
            "\n",
            "y_pred = classifier.predict_proba(\n",
            "    x_test,\n",
            ")\n",
            "\n",
            "accuracy = roc_auc_score(y_test, y_pred, multi_class=\"ovr\")\n",
            "print(\"Accuracy :\", accuracy)\n",
            "\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import pickle\n",
            "with open(\"../backend/trainedRF.pkl\", \"wb\") as f:\n",
            "    pickle.dump(classifier, f)"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.0"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
