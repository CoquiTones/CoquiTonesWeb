{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SLICE_SECONDS = 10 # Length of input slices for model.\n",
    "FFT_HOP_LENGTH = 512 # How many time domain samples per spectrogram frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# don\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import zipfile\n",
    "import csv\n",
    "import shutil\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,        \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script for extracting data from https://figshare.com/articles/dataset/Sounds_of_the_Eleutherodactylus_frog_community_from_Puerto_Rico/806302?file=3104183\n",
    "Unzips all the zips from root\n",
    "Simply Unzip downloaded file and provide path root of folder\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def extract_zip_file(file_path, extract_to):\n",
    "    \"\"\"\n",
    "    Extracts a single zip file to a specified directory.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the zip file.\n",
    "        extract_to (str): Path to the directory where the zip file will be extracted.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "        # print(f\"{os.path.basename(file_path)} extracted to {os.path.abspath(extract_to)}\")\n",
    "\n",
    "\n",
    "def extract_zip_files(zip_folder, extract_to):\n",
    "    \"\"\"\n",
    "    Extracts all zip files from a folder to a specified directory using threading.\n",
    "\n",
    "    Args:\n",
    "        zip_folder (str): Path to the folder containing zip files.\n",
    "        extract_to (str): Path to the directory where zip files will be extracted.\n",
    "    \"\"\"\n",
    "    # Make sure the extraction directory exists\n",
    "    os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "    data_file = \"FrequencyRange_by_species_and_site_Averages.csv\"\n",
    "    shutil.copyfile(os.path.join(zip_folder, data_file) , os.path.join(extract_to, data_file))\n",
    "\n",
    "    # List all zip files in the folder\n",
    "    zip_files = [\n",
    "        os.path.join(zip_folder, item)\n",
    "        for item in os.listdir(zip_folder)\n",
    "        if item.endswith(\".zip\")\n",
    "    ]\n",
    "\n",
    "    # Use ThreadPoolExecutor to extract zip files concurrently\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [\n",
    "            executor.submit(extract_zip_file, zip_file, extract_to)\n",
    "            for zip_file in zip_files\n",
    "        ]\n",
    "\n",
    "        # Wait for all futures to complete\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "    \n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/home/edwinc/Downloads/806302\"\n",
    "ExtractTo = \"/home/edwinc/Downloads/806302/Extracted\"\n",
    "extract_zip_files(filepath, ExtractTo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readAveragesData(path: str):\n",
    "\n",
    "    data = []\n",
    "    # Read the CSV file and store the data in a list of dictionaries\n",
    "    with open(path, \"r\") as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def prepare_csv(data_dir, output) -> None:\n",
    "\n",
    "    \n",
    "    averagesData = readAveragesData(\n",
    "        os.path.join(data_dir, \"FrequencyRange_by_species_and_site_Averages.csv\")\n",
    "    )\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # Iterate through each subfolder\n",
    "    for siteDataSet in os.listdir(data_dir):\n",
    "        site_folder = os.path.join(data_dir, siteDataSet)\n",
    "        if os.path.isdir(site_folder):\n",
    "            # example siteId  \"Site01-1\" such that the 4-6 index represents the site id; in this case 01\n",
    "            siteId = int(siteDataSet[4:6])\n",
    "            SiteData = [\n",
    "                averageClassification\n",
    "                for averageClassification in averagesData\n",
    "                if int(averageClassification[\"SiteID\"]) == siteId\n",
    "            ]\n",
    "\n",
    "            classifications = \", \".join(\n",
    "                [classification[\"Species\"] for classification in SiteData]\n",
    "            )\n",
    "            for audio_recording in os.listdir(site_folder):\n",
    "                if audio_recording.endswith(\".wav\"):\n",
    "                    audio_recording_abs_path = os.path.abspath(\n",
    "                        os.path.join(site_folder, audio_recording)\n",
    "                    )\n",
    "\n",
    "                    data.append([siteId, audio_recording_abs_path, classifications])\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        data,\n",
    "        columns=[\n",
    "            \"siteId\",\n",
    "            \"filename\",\n",
    "            \"species\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    df.to_csv(output, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = ExtractTo\n",
    "output = \"machine_learning/processed/processed.csv\"\n",
    "prepare_csv(data_dir, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_path):\n",
    "    \"\"\"\n",
    "    Extract spectrogram from audio file using librosa. \n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the audio file.\n",
    "\n",
    "    Returns:\n",
    "        np.array: Extracted features.\n",
    "        int: Sample rate in hertz\n",
    "    \"\"\"\n",
    "    audio, sr = librosa.load(file_path)\n",
    "\n",
    "    # MFCC\n",
    "    return librosa.feature.mfcc(y=audio, sr=sr, hop_length = FFT_HOP_LENGTH), sr\n",
    "\n",
    "\n",
    "def process_data(data_csv_path):\n",
    "    \"\"\"Read Csv with filename and generate spectrogram for each sample\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: dataframe with all data\n",
    "    \"\"\"\n",
    "    # data_csv_path = sys.argv[1]\n",
    "\n",
    "    df = pd.read_csv(data_csv_path)\n",
    "\n",
    "    # Initialize a list to store the results\n",
    "    spectrograms = []\n",
    "    sample_rates = []\n",
    "\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [\n",
    "            executor.submit(extract_features, row[\"filename\"])\n",
    "            for _, row in df.iterrows()\n",
    "        ]\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                spectrogram, sr = future.result()\n",
    "                spectrograms.append(spectrogram)\n",
    "                sample_rates.append(sr)\n",
    "            except Exception as exc:\n",
    "                print(f\"Generated an exception: {exc}\")\n",
    "\n",
    "    # Convert the list of spectrograms into a DataFrame\n",
    "    assert(min(sample_rates) == max(sample_rates))\n",
    "    sr = min(sample_rates)\n",
    "    slice_width = sr * SLICE_SECONDS // FFT_HOP_LENGTH\n",
    "    # Slice them into fixed widths\n",
    "    spectrogram_slices = []\n",
    "    for spectrogram in spectrograms:    \n",
    "        spectrogram = spectrogram[:, :-(spectrogram.shape[1] % slice_width)] # Take only the section of the spectrogram that will split into fixed slices\n",
    "        n_slices = spectrogram.shape[1] / slice_width\n",
    "        result = np.hsplit(spectrogram, n_slices)\n",
    "        spectrogram_slices.append(result)\n",
    "\n",
    "    # Group up the rows with slices. Since we have to duplicate the rows some number of times so they can each go with their own slice, we'll make a new df\n",
    "    master = []\n",
    "    for df_row, slices in zip(df.itertuples(index=False), spectrogram_slices):\n",
    "        for spectrogram in slices:\n",
    "            new_row = list(df_row) + list(spectrogram.flatten())\n",
    "            master.append(new_row)\n",
    "    # spectrogram_df = pd.DataFrame(spectrogram_slices)\n",
    "\n",
    "    # Concatenate the original DataFrame with the new DataFrame containing spectrograms\n",
    "    # df = pd.concat([df, spectrogram_df], axis=1)\n",
    "\n",
    "    return pd.DataFrame(master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = output\n",
    "df = process_data(csv_dir)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up readable index for df\n",
    "df.columns = pd.MultiIndex.from_arrays([   \n",
    "    ['metadata'] * 3 + ['spectral'] * (df.shape[1] - 3),\n",
    "    ['siteId', 'filename', 'species'] +\n",
    "    list(range(df.shape[1] - 3))\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.drop(\n",
    "    columns=[\"metadata\"]\n",
    ")  # Adjust this to include only feature columns\n",
    "# Convert all column names to strings\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"metadata\", \"species\"]\n",
    "\n",
    "# Encode the target labels as integers\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "le_bron = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "le_bron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=600, max_depth=18, min_samples_leaf=3)\n",
    "\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict_proba(\n",
    "    x_test,\n",
    ")\n",
    "\n",
    "accuracy = roc_auc_score(y_test, y_pred, multi_class=\"ovr\")\n",
    "print(\"Accuracy :\", accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./backend/trainedRF.pkl\", \"wb\") as f:\n",
    "    pickle.dump(classifier, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_test != discrete_pred) / len(y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
