{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Model Training Notebook\n",
            "\n",
            "This notebook can be used for downloading the [coqui acoustic dataset](https://figshare.com/articles/dataset/Sounds_of_the_Eleutherodactylus_frog_community_from_Puerto_Rico/806302) provided by researcher Luis Villanueva-Rivera and creating a machine learning model using Random Forest Classification algorithm. \n",
            "\n",
            "# Requirements\n",
            "\n",
            "Created python virtual environment with `backend\\requirements.txt` dependencies\n",
            "\n",
            "# Usage\n",
            "\n",
            "Play the cells one by one according to your needs. You may choose to skip certain steps if already done previously."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# -*- coding: utf-8 -*-\n",
            "\n",
            "# don\n",
            "import pandas as pd\n",
            "import numpy as np\n",
            "import librosa\n",
            "import os\n",
            "import zipfile\n",
            "import csv\n",
            "import functools\n",
            "import librosa\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.preprocessing import LabelEncoder\n",
            "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
            "from sklearn.metrics import (\n",
            "    roc_auc_score,\n",
            ")\n",
            "\n",
            "\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Download The Dataset\n",
            "\n",
            "If you have not already downloaded the dataset uncomment the `urlretrieve` line from the cell below\n",
            "\n",
            "Otherwise if you already have the zip folder, place it `machine_learning\\dataset\\coqui_dataset_Luis_Villanueva-Rivera.zip` and make sure to rename the zip folder accordingly. \n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from urllib.request import urlretrieve\n",
            "\n",
            "\n",
            "def extract_zip_file(file_path, extract_to):\n",
            "    \"\"\"\n",
            "    Extracts a single zip file to a specified directory.\n",
            "\n",
            "    Args:\n",
            "        file_path (str): Path to the zip file.\n",
            "        extract_to (str): Path to the directory where the zip file will be extracted.\n",
            "    \"\"\"\n",
            "    with zipfile.ZipFile(file_path, \"r\") as zip_ref:\n",
            "        zip_ref.extractall(extract_to)\n",
            "        print(\n",
            "            f\"{os.path.basename(file_path)} extracted to {os.path.abspath(extract_to)}\"\n",
            "        )\n",
            "\n",
            "\n",
            "def extract_zip_files(source_data_zip, destination_folder):\n",
            "    \"\"\"\n",
            "    Extracts all zip files from a folder to a specified directory using threading.\n",
            "\n",
            "    Args:\n",
            "        zip_folder (str): Path to the folder containing zip files.\n",
            "        extract_to (str): Path to the directory where zip files will be extracted.\n",
            "    \"\"\"\n",
            "    # Make sure the extraction directory exists\n",
            "    os.makedirs(destination_folder, exist_ok=True)\n",
            "\n",
            "    extract_zip_file(source_data_zip, destination_folder)\n",
            "    # List all zip files in the folder\n",
            "\n",
            "    zip_files = [\n",
            "        os.path.join(destination_folder, item)\n",
            "        for item in os.listdir(destination_folder)\n",
            "        if item.endswith(\".zip\")\n",
            "    ]\n",
            "\n",
            "    # Use ThreadPoolExecutor to extract zip files concurrently\n",
            "    with ThreadPoolExecutor() as executor:\n",
            "        futures = [\n",
            "            executor.submit(extract_zip_file, zip_file, destination_folder)\n",
            "            for zip_file in zip_files\n",
            "        ]\n",
            "\n",
            "        # Wait for all futures to complete\n",
            "        for future in futures:\n",
            "            future.result()\n",
            "\n",
            "    print(\"Done.\")\n",
            "\n",
            "\n",
            "def remove_zipped_files(extraction_destination_filepath):\n",
            "\n",
            "    zip_files = [\n",
            "        os.path.join(extraction_destination_filepath, item)\n",
            "        for item in os.listdir(extraction_destination_filepath)\n",
            "        if item.endswith(\".zip\")\n",
            "    ]\n",
            "\n",
            "    for file in zip_files:\n",
            "        os.remove(file)\n",
            "\n",
            "\n",
            "def download_if_necessary(dataset_source, source_data_filepath):\n",
            "\n",
            "    should_download = True\n",
            "    if os.path.exists(source_data_filepath):\n",
            "        should_replace = str(\n",
            "            input(\"Zipped file already downloaded, Do you wish to replace it? y/n\")\n",
            "        )\n",
            "        should_download = False if should_replace.lower() == \"n\" else True\n",
            "    else:\n",
            "        if not os.path.exists(\"dataset\"):\n",
            "            os.mkdir(\"dataset\")\n",
            "        print(\"Donwloading...\")\n",
            "\n",
            "    if should_download:\n",
            "        print(\"Donwloading Dataset to \", os.path.abspath(source_data_filepath))\n",
            "        urlretrieve(dataset_source, source_data_filepath)\n",
            "    else:\n",
            "        print(\"Skipping Download since it's already downloaded\")\n",
            "        \n",
            "\n",
            "\n",
            "\n",
            "extraction_destination_filepath = os.path.join(\"dataset\", \"extracted\")\n",
            "source_data_filepath = os.path.join(\n",
            "    \"dataset\", \"coqui_dataset_Luis_Villanueva-Rivera.zip\"\n",
            ")\n",
            "dataset_source = \"https://figshare.com/ndownloader/articles/806302/versions/18\"\n",
            "download_if_necessary(dataset_source, source_data_filepath)\n",
            "print(\"Donwloaded Dataset!\")\n",
            "extract_zip_files(source_data_filepath, extraction_destination_filepath)\n",
            "print(\"Extracted Zip Files!\")\n",
            "remove_zipped_files(extraction_destination_filepath)\n",
            "print(\"done!\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Data Preparation \n",
            "\n",
            "Generate a csv associating each audio sample to the corresponding coqui identified in it. A single file can have multiple coqui's identified. We use the `FrequencyRange_by_species_and_site_Averages.csv` to create this report"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "\n",
            "def clean_all_data_file(all_data_csv_path) -> pd.DataFrame:\n",
            "    with open(all_data_csv_path, mode=\"r\") as all_data_file:\n",
            "        reader = csv.DictReader(all_data_file)\n",
            "        root_df = pd.DataFrame.from_records(list(reader))\n",
            "        root_df = root_df.drop(columns=[None])\n",
            "        root_df.SoundID = pd.to_numeric(root_df.SoundID)\n",
            "        return root_df\n",
            "\n",
            "\n",
            "def match_sound_id_to_filename(zip_walk: list) -> pd.DataFrame:\n",
            "\n",
            "    # Now go through all the data.csv files to match the SoundID in our dataframe to a sound file\n",
            "    # Find all data.csv files\n",
            "\n",
            "    leaf_paths = [\n",
            "        os.path.join(trace[0], \"data.csv\")\n",
            "        for trace in zip_walk\n",
            "        if \"data.csv\" in trace[2]\n",
            "    ]\n",
            "    fragments = [\n",
            "        pd.read_csv(leaf_path)[[\"SoundID\", \"SiteID\", \"Filename\"]].drop_duplicates()\n",
            "        for leaf_path in leaf_paths\n",
            "    ]\n",
            "\n",
            "    soundid_to_filename_df = pd.concat(fragments)\n",
            "\n",
            "    return soundid_to_filename_df\n",
            "\n",
            "\n",
            "def update_return(d1: pd.DataFrame, d2: pd.DataFrame) -> pd.DataFrame:\n",
            "    d1.update(d2)\n",
            "    return d1\n",
            "\n",
            "\n",
            "def apply_abosolute_path_to_dataframe(\n",
            "    dataframe: pd.DataFrame, zip_walk: list\n",
            ") -> pd.DataFrame:\n",
            "\n",
            "    filename_dict = functools.reduce(\n",
            "        update_return,\n",
            "        [\n",
            "            {\n",
            "                filename: str(os.path.abspath(os.path.join(trace[0], filename)))\n",
            "                for filename in trace[2]\n",
            "                if filename.endswith(\".wav\")\n",
            "            }\n",
            "            for trace in zip_walk[1:]\n",
            "        ],\n",
            "    )\n",
            "\n",
            "    dataframe.Filename = dataframe.Filename.apply(lambda x: filename_dict[x])\n",
            "\n",
            "    return dataframe\n",
            "\n",
            "\n",
            "def merge_and_export_to_csv(\n",
            "    root_df: pd.DataFrame, processed_df_with_filename: pd.DataFrame\n",
            "):\n",
            "\n",
            "    # Now merge them and save the result\n",
            "    preprocess_final = pd.merge(\n",
            "        root_df, processed_df_with_filename, on=\"SoundID\", validate=\"many_to_one\"\n",
            "    )[[\"SiteID_x\", \"Filename\", \"Species\"]]\n",
            "    preprocess_final.columns = pd.Index([\"siteId\", \"filename\", \"species\"])\n",
            "\n",
            "    preprocess_final.siteId = pd.to_numeric(preprocess_final.siteId)\n",
            "\n",
            "    species_classes = list(preprocess_final[\"species\"].unique())\n",
            "\n",
            "    final_output = preprocess_final[[\"siteId\", \"filename\"]].drop_duplicates()\n",
            "    for species in species_classes:\n",
            "        # For each species, make a new column in the dataframe that says which files contain that species\n",
            "        final_output[species] = [\n",
            "            species\n",
            "            in set(\n",
            "                preprocess_final[preprocess_final[\"filename\"] == filename][\"species\"]\n",
            "            )\n",
            "            for filename in final_output[\"filename\"]\n",
            "        ]\n",
            "\n",
            "    if (not os.path.exists(\"processed\")):\n",
            "        os.mkdir(\"processed\")\n",
            "    \n",
            "    output_path = \"processed/processed.csv\"\n",
            "    final_output.to_csv(output_path)\n",
            "\n",
            "        \n",
            "\n",
            "    return os.path.abspath(\"processed/processed.csv\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "extraction_destination_filepath = os.path.join(\"dataset\", \"extracted\")\n",
            "all_data_file_path: str = os.path.join(\n",
            "    extraction_destination_filepath, \"FrequencyRange_by_species_and_site_AllData.csv\"\n",
            ")\n",
            "\n",
            "root_df: pd.DataFrame = clean_all_data_file(all_data_file_path)\n",
            "zip_walk = list(os.walk(extraction_destination_filepath))\n",
            "soundid_to_filename: pd.DataFrame = match_sound_id_to_filename(zip_walk)\n",
            "dataframe_with_absoule_path_in_filename: pd.DataFrame = apply_abosolute_path_to_dataframe(soundid_to_filename, zip_walk)\n",
            "pre_processed_data_csv_filepath = merge_and_export_to_csv(root_df, dataframe_with_absoule_path_in_filename)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Data Processing\n",
            "\n",
            "Generate a Spectrogram for every audio sample. "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "SLICE_SECONDS = 10  # Length of input slices for model.\n",
            "FFT_HOP_LENGTH = 512  # How many time domain samples per spectrogram frame\n",
            "SAMPLE_RATE = 22050\n",
            "Y_RESOLUTION = 20\n",
            "\n",
            "n_model_input_parameters = SAMPLE_RATE // FFT_HOP_LENGTH * SLICE_SECONDS * Y_RESOLUTION\n",
            "\n",
            "f\"Model takes {n_model_input_parameters} parameters\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "df = pd.read_csv(str(pre_processed_data_csv_filepath))\n",
            "df = df.drop(columns=\"Unnamed: 0\")\n",
            "df[\"E. coqui\"] = df[\"E. coqui - co\"]\n",
            "df[\"E. portoricensis\"] = df[\"E. portoricensis - co\"]\n",
            "df = df.drop(\n",
            "    columns=[\n",
            "        \"E. coqui - co\",\n",
            "        \"E. coqui - qui\",\n",
            "        \"E. portoricensis - co\",\n",
            "        \"E. portoricensis - qui\",\n",
            "    ]\n",
            ")\n",
            "df = df[\n",
            "    [\n",
            "        \"siteId\",\n",
            "        \"filename\",\n",
            "        \"E. coqui\",\n",
            "        \"E. wightmanae\",\n",
            "        \"E. gryllus\",\n",
            "        \"E. portoricensis\",\n",
            "        \"E. unicolor\",\n",
            "        \"E. hedricki\",\n",
            "        \"E. locustus\",\n",
            "        \"E. richmondi\",\n",
            "    ]\n",
            "]\n",
            "df"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def extract_features(file_path):\n",
            "    \"\"\"\n",
            "    Extract spectrogram from audio file using librosa.\n",
            "\n",
            "    Args:\n",
            "        file_path (str): Path to the audio file.\n",
            "\n",
            "    Returns:\n",
            "        np.array: Extracted features.\n",
            "        int: Sample rate in hertz\n",
            "    \"\"\"\n",
            "    audio, sr = librosa.load(file_path)\n",
            "\n",
            "    # MFCC\n",
            "    return librosa.feature.mfcc(y=audio, sr=sr, hop_length=FFT_HOP_LENGTH), sr\n",
            "\n",
            "\n",
            "# Initialize a list to store the results\n",
            "spectrograms = []\n",
            "sample_rates = []\n",
            "\n",
            "with ThreadPoolExecutor() as executor:\n",
            "    futures = [\n",
            "        executor.submit(extract_features, row[\"filename\"]) for _, row in df.iterrows()\n",
            "    ]\n",
            "\n",
            "    for future in as_completed(futures):\n",
            "        try:\n",
            "            spectrogram, sr = future.result()\n",
            "            spectrograms.append(spectrogram)\n",
            "            sample_rates.append(sr)\n",
            "        except Exception as exc:\n",
            "            print(f\"Generated an exception: {exc}\")\n",
            "\n",
            "# Process the spectrograms\n",
            "assert min(sample_rates) == max(sample_rates)\n",
            "sr = min(sample_rates)\n",
            "slice_width = sr * SLICE_SECONDS // FFT_HOP_LENGTH\n",
            "# Slice them into fixed widths\n",
            "spectrogram_slices = []\n",
            "for spectrogram in spectrograms:\n",
            "    spectrogram = spectrogram[\n",
            "        :, : -(spectrogram.shape[1] % slice_width)\n",
            "    ]  # Take only the section of the spectrogram that will split into fixed slices\n",
            "    n_slices = spectrogram.shape[1] / slice_width\n",
            "    result = np.hsplit(spectrogram, n_slices)\n",
            "    spectrogram_slices.append(result)\n",
            "\n",
            "# Group up the filenames with their corresponding spectrogram slices\n",
            "spectral_data = pd.DataFrame(\n",
            "    sum(\n",
            "        [\n",
            "            [(filename,) + tuple(spectrogram.flatten()) for spectrogram in spectrograms]\n",
            "            for spectrograms, filename in zip(spectrogram_slices, df.filename)\n",
            "        ],\n",
            "        [],\n",
            "    )\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Get the names of the species\n",
            "species_names = list(df.columns.drop([\"siteId\", \"filename\"]))\n",
            "# Join the new spectrogram data with the existing dataframe\n",
            "df = df.merge(spectral_data, left_on=\"filename\", right_on=0, how=\"right\").drop(\n",
            "    columns=0\n",
            ")\n",
            "df"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Set up readable index for df\n",
            "df.columns = pd.MultiIndex.from_arrays(\n",
            "    [\n",
            "        [\"metadata\"] * 2\n",
            "        + [\"classes\"] * len(species_names)\n",
            "        + [\"spectral\"] * (n_model_input_parameters),\n",
            "        [\"siteId\", \"filename\"] + species_names + list(range(n_model_input_parameters)),\n",
            "    ]\n",
            ")\n",
            "df"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "x = df[\"spectral\"]  # Adjust this to include only feature columns\n",
            "# Convert all column names to strings\n",
            "\n",
            "x"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "y = df[\"classes\"].applymap(int)\n",
            "y"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
            "\n",
            "classifier = RandomForestClassifier(\n",
            "    n_estimators=600, max_depth=25, min_samples_leaf=3, n_jobs=-1\n",
            ")\n",
            "\n",
            "classifier.fit(x_train, y_train)\n",
            "\n",
            "y_pred = classifier.predict(\n",
            "    x_test,\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "y_test"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "prediction_df = pd.DataFrame(y_pred, columns=y_test.columns, index=y_test.index)\n",
            "\n",
            "prediction_df"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "accuracy_df = prediction_df == y_test\n",
            "accuracy_df.sum() / 69"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import pickle\n",
            "model_backend_path = os.path.abspath(os.path.join(os.path.dirname(os.getcwd()), \"backend\", \"trainedRF.pkl\"))\n",
            "with open(model_backend_path, \"wb\") as f:\n",
            "    pickle.dump(classifier, f)"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": ".venv",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.0"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
